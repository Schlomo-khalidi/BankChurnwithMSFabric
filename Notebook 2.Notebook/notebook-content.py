# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "e1a48a91-6d93-4f1e-a7f6-b4e8af15061e",
# META       "default_lakehouse_name": "BankML",
# META       "default_lakehouse_workspace_id": "91c7377b-689e-4b2f-b71b-902689c7c335",
# META       "known_lakehouses": [
# META         {
# META           "id": "e1a48a91-6d93-4f1e-a7f6-b4e8af15061e"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

df = (
    spark.read.option("header", True)
    .option("inferSchema", True)
    .csv("Files/churn/raw/churn.csv")
    .cache()
)
display(df)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df = df.toPandas()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

import seaborn as sns
sns.set_theme(style="whitegrid", palette="tab10", rc = {'figure.figsize':(9,6)})
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from matplotlib import rc, rcParams
import numpy as np
import pandas as pd
import itertools

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df, summary=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for pandas DataFrame

def clean_data(df):
    # Drop duplicate rows in columns: 'RowNumber', 'CustomerId'
    df = df.drop_duplicates(subset=['RowNumber', 'CustomerId'])
    # Drop rows with missing data across all columns
    df = df.dropna()
    # Drop columns: 'RowNumber', 'CustomerId', 'Surname'
    df = df.drop(columns=['RowNumber', 'CustomerId', 'Surname'])
    return df

df_clean = clean_data(df.copy())
display(df_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# # **Exploratory Data Analysis (EDA)**

# MARKDOWN ********************

# ###### **Determine Categorical, numerical and target attributes**

# CELL ********************

#Determine the dependent (target) attribute
dependent_variable_name = 'Exited'
print(dependent_variable_name)

#Determinr The categotical attribute
categorical_variables = [col for col in df_clean.columns if col in "O" or df_clean[col].nunique() <=5 and col not in 'Exited']
print(categorical_variables)

#Determinr the numerical attributes
numeric_variables = [col for col in df_clean.columns if df_clean[col].dtype != 'object' and  df_clean[col].nunique() > 5]
print(numeric_variables)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **The five-number summary of numerical attributes (minimum score, first quartile, median, second quartile, maximum score)**

# CELL ********************

df_num_cols = df_clean[numeric_variables]
sns.set(font_scale = 0.7) 
fig, axes = plt.subplots(nrows = 2, ncols = 3, gridspec_kw = dict(hspace=0.3),
figsize = (17,8))
fig.tight_layout()
for ax,col in zip(axes.flatten(), df_num_cols.columns):
 sns.boxplot(x = df_num_cols[col], color='green', ax = ax)
fig.delaxes(axes[1,2])

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Distribution of exited and non exited customers across the categorical attributes**

# CELL ********************

attr_list = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 
'NumOfProducts', 'Tenure']
df_clean['Exited'] = df_clean['Exited'].astype(str)
fig, axarr = plt.subplots(2, 3, figsize=(15, 4))
for ind, item in enumerate (attr_list):
 sns.countplot(x = item, hue = 'Exited', data = df_clean, ax = axarr[ind%2]
[ind//2])
fig.subplots_adjust(hspace=0.7)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Distribution of numerical attributes**

# CELL ********************

columns = df_num_cols.columns[: len(df_num_cols.columns)]
fig = plt.figure()
fig.set_size_inches(18, 8)
length = len(columns)
for i,j in itertools.zip_longest(columns, range(length)):
 plt.subplot((length // 2), 3, j+1)
 plt.subplots_adjust(wspace = 0.2, hspace = 0.5)
 df_num_cols[i].hist(bins = 20, edgecolor = 'black')
 plt.title(i)
plt.show()


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# #

# MARKDOWN ********************

# ###### **Perform Feature engineering**

# CELL ********************

df_clean['Tenure'] = df_clean['Tenure'].astype(int)
df_clean["NewTenure"] = df_clean["Tenure"]/df_clean["Age"]
df_clean["NewCreditsScore"] = pd.qcut(df_clean['CreditScore'], 6, labels = [1, 2, 
3, 4, 5, 6])
df_clean["NewAgeScore"] = pd.qcut(df_clean['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 
7, 8])
df_clean["NewBalanceScore"] = pd.qcut(df_clean['Balance'].rank(method="first"), 5,
labels = [1, 2, 3, 4, 5])
df_clean["NewEstSalaryScore"] = pd.qcut(df_clean['EstimatedSalary'], 10, labels =
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Performing one-hot encoding using data wrangler**

# CELL ********************

# Code generated by Data Wrangler for pandas DataFrame

import pandas as pd

def clean_data(df_clean):
    # One-hot encode columns: 'Gender', 'Geography'
    for column in ['Gender', 'Geography']:
        insert_loc = df_clean.columns.get_loc(column)
        df_clean = pd.concat([df_clean.iloc[:,:insert_loc], pd.get_dummies(df_clean.loc[:, [column]]), df_clean.iloc[:,insert_loc+1:]], axis=1)
    return df_clean

df_clean_1 = clean_data(df_clean.copy())
display(df_clean_1)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# #### **Summary of observations from the exploratory data analysis**
# - Most of the customers are from France comparing to Spain and Germany, while Spain has the lowest churn rate comparing to France and Germany.
# - Most of the customers have credit cards.
# - There are customers whose age and credit score are above 60 and below 400, respectively, but they can't be considered as outliers.
# - Very few customers have more than two of the bank's products.
# - Customers who aren't active have a higher churn rate.
# - Gender and tenure years don't seem to have an impact on customer's decision to close the bank account

# CELL ********************

SparkDF = spark.createDataFrame(df_clean_1)
SparkDF.write.mode('overwrite').format('delta').save('Tables/df_clean')

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## **Train and Register Machine Learning Models**

# CELL ********************

%pip install imblearn

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

import pandas as pd
SEED = 12345
df_clean = spark.read.format('delta').load('Tables/df_clean').toPandas()
display(df_clean, summary=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Generating Experiment to track and load the model using MLFlow**

# CELL ********************

import mlflow
#setup the experiment name
EXPERIMENT_NAME = 'bank-churn-experiment'
#set experiment and autologing capabilities
mlflow.set_experiment(EXPERIMENT_NAME)
mlflow.autolog(exclusive=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, classification_report, roc_auc_score, precision_score

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Prepare training, validation and test datasets**

# CELL ********************

y = df_clean['Exited']
X = df_clean.drop('Exited', axis=1)
#split the dataset to 60%, 20%, 20% for training, validation and test datasets respectively
#train_test separation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)
#train_validation separation
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED) 

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Save the test data to a delta table**

# CELL ********************

table_name = 'df_test'
df_test = spark.createDataFrame(X_test)
df_test.write.mode('overwrite').format('delta').save(f'Tables/{table_name}')
print(f'test data saved in {table_name}')

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Apply SMOTE(Synthetic Minority Oversampling Technique) to the training data to synthesize new samples from the minority class**

# CELL ********************

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=SEED)
X_res, y_res = sm.fit_resample(X_train, y_train)
new_train = pd.concat([X_res, y_res], axis=1)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Model Training**

# MARKDOWN ********************

# Train the model using Random Forest with a maximum of 4 and 4 features

# CELL ********************

#Regidter the training model with autologging
mlflow.sklearn.autolog(registered_model_name='rfc1_sm')
rfc1_sm = RandomForestClassifier(max_depth=4, max_features=4, min_samples_split=3, random_state=1)
with mlflow.start_run(run_name='rfc1_sm') as run:
    rfc1_sm_run_id = run.info.run_id #Capture run_id for later model prediction
    print('run_id: {}; Status: {}'.format(rfc1_sm_run_id, run.info.status))
    rfc1_sm.fit(X_res, y_res.ravel())
    rfc1_sm.score(X_val, y_val)
    ypred = rfc1_sm.predict(X_val)
    ct_rfc1_sm = classification_report(y_val, ypred)
    cm_rfc1_sm = confusion_matrix(y_val, ypred)
    roc_auc_rfc1_sm = roc_auc_score(y_res, rfc1_sm.predict_proba(X_res)[:,1])

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# Train the model using Random Forest with a maximum of 8 and 6 features

# CELL ********************

mlflow.sklearn.autolog(registered_model_name='rfc2_sm')
rfc2_sm = RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=3, random_state=1)
with mlflow.start_run(run_name='rfc2_sm') as run:
    rfc2_sm_run_id = run.info.run_id #Capture run_id for later model prediction
    print('run_id: {}; Status: {}'.format(rfc2_sm_run_id, run.info.status))
    rfc2_sm.fit(X_res, y_res.ravel())
    rfc2_sm.score(X_val, y_val)
    ypred = rfc2_sm.predict(X_val)
    ct_rfc2_sm = classification_report(y_val, ypred)
    cm_rfc2_sm = confusion_matrix(y_val, ypred)
    roc_auc_rfc2_sm = roc_auc_score(y_res, rfc2_sm.predict_proba(X_res)[:,1])

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Train the model  using LIGHTGBM**

# CELL ********************

import mlflow.pyfunc
import pandas as pd

class LGBMClassifierWrapper(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        import mlflow.lightgbm
        self.model = mlflow.lightgbm.load_model(context.artifacts["lgbm_model"])

    def predict(self, context, model_input):
        # Use predict for labels
        result = self.model.predict(model_input)
        # Cast output explicitly to integer type
        return pd.Series(result.astype(int))


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#lightgbm model
import mlflow
from mlflow.models import infer_signature

with mlflow.start_run(run_name='lgbm_sm_wrapped') as run:
    # Train the LightGBM model
    lgbm_sm_model.fit(X_res, y_res.ravel())

    # Log the underlying LightGBM model
    mlflow.lightgbm.log_model(
        lgbm_sm_model,
        artifact_path="lgbm_model"
    )

    # Create and log the wrapped pyfunc model
    sample_input = X_res[:10]  # or X_res.sample(10)
    sample_output = lgbm_sm_model.predict(sample_input)
    signature = infer_signature(sample_input, pd.Series(sample_output.astype(int)))

    mlflow.pyfunc.log_model(
        artifact_path="model",
        python_model=LGBMClassifierWrapper(),
        artifacts={"lgbm_model": mlflow.get_artifact_uri("lgbm_model")},
        signature=signature,
        registered_model_name="lgbm_sm_wrapped"
    )



# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Experiment artifact for tracking model performance**

# MARKDOWN ********************

# ###### **Assess the performance of the trained datasets on the performance datasets**

# CELL ********************

# Define run_uri to fetch the model
# mlflow client: mlflow.model.url, list model
load_model_rfc1_sm = mlflow.sklearn.load_model(f"runs:/{rfc1_sm_run_id}/model")
load_model_rfc2_sm = mlflow.sklearn.load_model(f"runs:/{rfc2_sm_run_id}/model")
load_model_lgbm1_sm = mlflow.lightgbm.load_model(f"runs:/{lgbm_sm_run_id}/model")
# Assess the performance of the loaded model on validation dataset
ypred_rfc1_sm_v1 = load_model_rfc1_sm.predict(X_val) # Random Forestwith max depth of 4 and 4 features
ypred_rfc2_sm_v1 = load_model_rfc2_sm.predict(X_val) # Random Forestwith max depth of 8 and 6 features
ypred_lgbm1_sm_v1 = load_model_lgbm1_sm.predict(X_val) # LightGBM


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### **Show the True/false Positives/negatives using the confusion matrix**

# CELL ********************

import seaborn as sns
sns.set_theme(style="whitegrid", palette="tab10", rc = {'figure.figsize':(9,6)})
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from matplotlib import rc, rcParams
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    print(cm)
    plt.figure(figsize=(4,4))
    plt.rcParams.update({'font.size': 10})
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45, color="blue")
    plt.yticks(tick_marks, classes, color="blue")

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="red" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#Confusion Matrix for Random Forest Classifier with maximum depth of 4 and 4 features
cfm = confusion_matrix(y_val, y_pred=ypred_rfc1_sm_v1)
plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],
                      title='Random Forest with max depth of 4')
tn, fp, fn, tp = cfm.ravel()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#Confusion Matrix for Random Forest Classifier with maximum depth of 8 and 6 features
cfm = confusion_matrix(y_val, y_pred=ypred_rfc2_sm_v1)
plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],
                      title='Random Forest with max depth of 8')
tn, fp, fn, tp = cfm.ravel()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#Confusion Matrix for LightGBM
cfm = confusion_matrix(y_val, y_pred=ypred_lgbm1_sm_v1)
plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],
                      title='LightGBM')
tn, fp, fn, tp = cfm.ravel()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Perform Batch scoring and save predictions to a LH**

# CELL ********************

df_test = spark.read.format('delta').load('Tables/df_test')
display(df_test)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **PRDICT with the transformer API**

# CELL ********************

from synapse.ml.predict import MLFlowTransformer



# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

model = MLFlowTransformer(
    inputCols=list(df_test.columns),
    outputCol='predictions',
    modelName='lgbm_sm_wrapped',
    modelVersion=1
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

import pandas

predictions = model.transform(df_test)
display(predictions)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Save predictions to lakehouse to be used for generating a Power BI report
table_name = "customer_churn_test_predictions"
predictions.write.format('delta').mode("overwrite").save(f"Tables/{table_name}")
print(f"Spark DataFrame saved to delta table: {table_name}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
